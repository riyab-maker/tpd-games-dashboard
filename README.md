# Matomo Events Dashboard

A Streamlit dashboard for analyzing Matomo events data with optimized deployment for Render.

## ğŸ—ï¸ Architecture

This project uses a **preprocessing architecture** to handle large datasets efficiently on Render's 512MB memory limit:

- **Local Preprocessing**: All data processing happens locally using `preprocess_data.py`
- **Lightweight Dashboard**: The Streamlit dashboard only handles visualization of preprocessed data
- **Automated Updates**: GitHub Actions run daily to update data and trigger redeployment

## ğŸ“ Project Structure

```
â”œâ”€â”€ preprocess_data.py          # Data preprocessing script (run locally)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ streamlit_dashboard.py  # Lightweight visualization dashboard
â”œâ”€â”€ data/                       # Processed data files (generated by preprocessing)
â”‚   â”œâ”€â”€ processed_data.csv
â”‚   â”œâ”€â”€ summary_data.csv
â”‚   â”œâ”€â”€ score_distribution_data.csv
â”‚   â”œâ”€â”€ time_series_data.csv
â”‚   â”œâ”€â”€ repeatability_data.csv
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ daily-preprocessing.yml # Automated daily data updates
â””â”€â”€ README.md
```

## ğŸš€ Setup and Deployment

### 1. Local Development

1. **Install dependencies**:
   ```bash
   pip install pandas pymysql python-dotenv streamlit altair
   ```

2. **Set up environment variables** (create `.env` file from `env.template`):
   ```bash
   cp env.template .env
   # Edit .env with your database credentials
   ```

3. **Run preprocessing** (required before deployment):
   ```bash
   python preprocess_data.py
   ```

4. **Test dashboard locally**:
   ```bash
   streamlit run src/streamlit_dashboard.py
   ```

### 2. Render Deployment

1. **Connect your GitHub repository to Render**

2. **Set up environment variables in Render**:
   - `DB_HOST`: Your database host
   - `DB_PORT`: 3310
   - `DB_NAME`: Your database name
   - `DB_USER`: Your database user
   - `DB_PASSWORD`: Your database password

3. **Configure GitHub Secrets** (for automated updates):
   - Go to your repository â†’ Settings â†’ Secrets and variables â†’ Actions
   - Add the same database credentials as secrets:
     - `DB_HOST`
     - `DB_PORT`
     - `DB_NAME`
     - `DB_USER`
     - `DB_PASSWORD`

4. **Deploy**: Render will automatically deploy when you push to your repository

## ğŸ”„ Daily Data Updates

The project includes automated daily data preprocessing:

- **GitHub Action**: Runs daily at 2 AM UTC
- **Automatic Updates**: Fetches latest data, processes it, and pushes to repository
- **Auto-Deploy**: Render automatically redeploys with updated data

### Manual Trigger

You can also trigger preprocessing manually:
1. Go to your repository â†’ Actions
2. Select "Daily Data Preprocessing"
3. Click "Run workflow"

## ğŸ“Š Data Processing

The preprocessing script (`preprocess_data.py`) performs all heavy data operations:

- **Database Queries**: Fetches data from multiple SQL queries
- **Data Transformation**: Processes JSON data, calculates scores, handles game sessions
- **Aggregation**: Builds summary statistics, time series, and repeatability analysis
- **Score Distribution**: Complex scoring logic for different game types
- **Time Series**: Multiple time period aggregations (day, week, month)

## ğŸ¯ Dashboard Features

The lightweight dashboard provides:

- **Conversion Funnels**: Users, Visits, and Instances
- **Score Distribution**: Detailed analysis by game and score
- **Repeatability Analysis**: User engagement patterns
- **Time Series**: Trends over different time periods
- **Interactive Filters**: Game selection and date ranges

## âš ï¸ Important Notes

1. **Preprocessing Required**: Always run `python preprocess_data.py` before deployment
2. **Memory Optimization**: Dashboard only loads preprocessed CSV files
3. **Database Access**: Preprocessing requires database access (local or CI/CD)
4. **Data Freshness**: Data is updated daily via GitHub Actions

## ğŸ› ï¸ Troubleshooting

### Dashboard Shows "Missing processed data files"
- Run `python preprocess_data.py` locally
- Ensure all data files are committed to the repository

### Memory Issues on Render
- Verify that preprocessing is working correctly
- Check that all data files are present in the `data/` directory
- Ensure the dashboard is only loading CSV files, not querying the database

### GitHub Action Fails
- Check that all database secrets are properly set
- Verify database connectivity from GitHub Actions
- Review the Action logs for specific error messages

## ğŸ“ˆ Performance Benefits

- **Memory Usage**: Dashboard uses <100MB on Render (vs 512MB+ with direct DB queries)
- **Load Speed**: Preprocessed data loads instantly
- **Reliability**: No database connection issues on Render
- **Scalability**: Can handle larger datasets without memory constraints

## ğŸ”§ Development

### Adding New Visualizations

1. Add processing logic to `preprocess_data.py`
2. Save results to a new CSV file in the `data/` directory
3. Add visualization code to `src/streamlit_dashboard.py`
4. Update the `REQUIRED_FILES` list in the dashboard

### Modifying Data Processing

1. Update the processing logic in `preprocess_data.py`
2. Test locally with `python preprocess_data.py`
3. Commit and push changes
4. The GitHub Action will automatically update data daily

## ğŸ“ License

This project is part of the Matomo Events Dashboard system.